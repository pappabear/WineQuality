dataset.red.wines <- read.csv('winequality-red.csv', sep = ";")
dataset.red.wines$quality[which(dataset.red.wines$quality == 3 | dataset.red.wines$quality == 4)] <- 'poor'
dataset.red.wines$quality[which(dataset.red.wines$quality == 5 | dataset.red.wines$quality == 6)] <- 'ok'
dataset.red.wines$quality[which(dataset.red.wines$quality == 7 | dataset.red.wines$quality == 8)] <- 'great'
dataset.red.wines$quality <- as.factor(dataset.red.wines$quality)
set.seed(100)
# Step 1: get row numbers for the training data
training.row.numbers <- createDataPartition(dataset.red.wines$quality, p=0.8, list=FALSE)
# Step 2: create the training  dataset
dataset.training <- dataset.red.wines[training.row.numbers,]
# Step 3: create the test dataset
dataset.testing <- dataset.red.wines[-training.row.numbers,]
str(dataset.training)
# Define the training control - this problem has THREE classes so ROC is not available
fit.control <- trainControl(
method = 'cv',                   # k-fold cross validation
number = 5,                      # number of folds
savePredictions = 'final',       # saves predictions for optimal tuning parameter
classProbs = T#,                  # should class probabilities be returned
#summaryFunction=twoClassSummary  # results summary function
)
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
trControl = fit.control)
model.rf
# show the significance of predictors, in order
varImp(model.rf)
set.seed(100)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
trControl = fit.control)
model.svm
# show the significance of predictors, in order
varImp(model.svm)
# train k-nearest neighbors
model.knn = train(quality ~ .,
data=dataset.training,
method='knn',
tuneLength=5,
trControl=fit.control)
model.knn
varImp(model.knn)
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
trControl=fit.control)
model.nb
# Compare model performances using resample()
models.compared <- resamples(list(`Random Forest`=model.rf,
`SVM`=model.svm,
`kNN`=model.knn,
`Naive Bayes`=model.nb
))
# Summary of the models performances
summary(models.compared)
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
preProcess = c("center", "scale"),
trControl = fit.control)
model.rf
set.seed(100)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
preProcess = c("center", "scale"),
trControl = fit.control)
model.svm
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
preProcess = c("center", "scale"),
trControl=fit.control)
model.nb
# apply capping to dataset.training$fixed.acidity
qnt <- quantile(dataset.training$fixed.acidity, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$fixed.acidity, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$fixed.acidity, na.rm = T)
dataset.training$fixed.acidity[dataset.training$fixed.acidity < (qnt[1] - H)] <- caps[1]
dataset.training$fixed.acidity[dataset.training$fixed.acidity > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$volatile.acidity
qnt <- quantile(dataset.training$volatile.acidity, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$volatile.acidity, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$volatile.acidity, na.rm = T)
dataset.training$volatile.acidity[dataset.training$volatile.acidity < (qnt[1] - H)] <- caps[1]
dataset.training$volatile.acidity[dataset.training$volatile.acidity > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$citric.acid
qnt <- quantile(dataset.training$citric.acid, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$citric.acid, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$citric.acid, na.rm = T)
dataset.training$citric.acid[dataset.training$citric.acid < (qnt[1] - H)] <- caps[1]
dataset.training$citric.acid[dataset.training$citric.acid > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$residual.sugar
qnt <- quantile(dataset.training$citric.acid, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$citric.acid, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$citric.acid, na.rm = T)
dataset.training$citric.acid[dataset.training$citric.acid < (qnt[1] - H)] <- caps[1]
dataset.training$citric.acid[dataset.training$citric.acid > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$chlorides
qnt <- quantile(dataset.training$chlorides, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$chlorides, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$chlorides, na.rm = T)
dataset.training$chlorides[dataset.training$chlorides < (qnt[1] - H)] <- caps[1]
dataset.training$chlorides[dataset.training$chlorides > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$free.sulfur.dioxide
qnt <- quantile(dataset.training$free.sulfur.dioxide, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$free.sulfur.dioxide, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$free.sulfur.dioxide, na.rm = T)
dataset.training$free.sulfur.dioxide[dataset.training$free.sulfur.dioxide < (qnt[1] - H)] <- caps[1]
dataset.training$free.sulfur.dioxide[dataset.training$free.sulfur.dioxide > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$total.sulfur.dioxide
qnt <- quantile(dataset.training$total.sulfur.dioxide, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$total.sulfur.dioxide, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$total.sulfur.dioxide, na.rm = T)
dataset.training$total.sulfur.dioxide[dataset.training$total.sulfur.dioxide < (qnt[1] - H)] <- caps[1]
dataset.training$total.sulfur.dioxide[dataset.training$total.sulfur.dioxide > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$density
qnt <- quantile(dataset.training$density, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$density, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$density, na.rm = T)
dataset.training$density[dataset.training$density < (qnt[1] - H)] <- caps[1]
dataset.training$density[dataset.training$density > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$pH
qnt <- quantile(dataset.training$pH, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$pH, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$pH, na.rm = T)
dataset.training$pH[dataset.training$pH < (qnt[1] - H)] <- caps[1]
dataset.training$pH[dataset.training$pH > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$sulphates
qnt <- quantile(dataset.training$sulphates, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$sulphates, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$sulphates, na.rm = T)
dataset.training$sulphates[dataset.training$sulphates < (qnt[1] - H)] <- caps[1]
dataset.training$sulphates[dataset.training$sulphates > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$alcohol
qnt <- quantile(dataset.training$alcohol, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$alcohol, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$alcohol, na.rm = T)
dataset.training$alcohol[dataset.training$alcohol < (qnt[1] - H)] <- caps[1]
dataset.training$alcohol[dataset.training$alcohol > (qnt[2] + H)] <- caps[2]
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset.red.wines)[i])
}
# boxplots
par(mfrow=c(1,5))
for(i in 7:11) {
boxplot(x[,i], main=names(dataset.red.wines)[i])
}
# re-build the models with the capped predictors
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
trControl = fit.control)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
trControl = fit.control)
# train k-nearest neighbors
model.knn = train(quality ~ .,
data=dataset.training,
method='knn',
tuneLength=5,
trControl=fit.control)
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
trControl=fit.control)
# Compare model performances using resample()
models.compared <- resamples(list(`Random Forest`=model.rf,
`SVM`=model.svm,
`kNN`=model.knn,
`Naive Bayes`=model.nb
))
# Summary of the models performances
summary(models.compared)
#install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth'))
#install.packages('e1071')
#install.packages('kernlab')
#install.packages('ellipse')
#install.packages('PerformanceAnalytics')
#install.packages('klaR')
#install.packages('outliers')
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset.training)[i])
}
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(dataset.training[,i], main=names(dataset.training)[i])
}
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(dataset.training[,i], main=names(dataset.training)[i])
}
# boxplots
par(mfrow=c(1,5))
for(i in 7:11) {
boxplot(dataset.training[,i], main=names(dataset.training)[i])
}
options(warn=-1)
library(caret)
library(dplyr)
library(skimr)
library(RANN)
library(e1071)
library(PerformanceAnalytics)
library(outliers)
dataset.white.wines <- read.csv('winequality-white.csv', sep = ";")
dataset.red.wines <- read.csv('winequality-red.csv', sep = ";")
str(dataset.red.wines)
head(dataset.red.wines)
skim_to_wide(dataset.red.wines)
summary(dataset.red.wines)
x = dataset.red.wines[,1:11]
y = dataset.red.wines$quality
chart.Correlation(x, bg=y)
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(x[,i], main=names(dataset.red.wines)[i])
}
par(mfrow=c(1,5))
for(i in 7:11) {
boxplot(x[,i], main=names(dataset.red.wines)[i])
}
ggplot(dataset.red.wines, aes(x=alcohol)) +
geom_density() +
geom_vline(aes(xintercept=mean(alcohol)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=fixed.acidity)) +
geom_density() +
geom_vline(aes(xintercept=mean(fixed.acidity)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=volatile.acidity)) +
geom_density() +
geom_vline(aes(xintercept=mean(volatile.acidity)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=citric.acid)) +
geom_density() +
geom_vline(aes(xintercept=mean(citric.acid)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=residual.sugar)) +
geom_density() +
geom_vline(aes(xintercept=mean(residual.sugar)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=chlorides)) +
geom_density() +
geom_vline(aes(xintercept=mean(chlorides)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=free.sulfur.dioxide)) +
geom_density() +
geom_vline(aes(xintercept=mean(free.sulfur.dioxide)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=total.sulfur.dioxide)) +
geom_density() +
geom_vline(aes(xintercept=mean(total.sulfur.dioxide)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=density)) +
geom_density() +
geom_vline(aes(xintercept=mean(density)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=pH)) +
geom_density() +
geom_vline(aes(xintercept=mean(pH)),
color="blue", linetype="dashed", size=1)
ggplot(dataset.red.wines, aes(x=sulphates)) +
geom_density() +
geom_vline(aes(xintercept=mean(sulphates)),
color="blue", linetype="dashed", size=1)
#set.seed(100)
#ctrl <- rfeControl(functions = rfFuncs,
#                   method = "repeatedcv",
#                   repeats = 5,
#                   verbose = FALSE)
#lmProfile <- rfe(x=dataset.red.wines[, 1:11], y=dataset.red.wines$quality, rfeControl=ctrl)
#lmProfile
# convert the quality attribute to a real 3-level factor
dataset.red.wines <- read.csv('winequality-red.csv', sep = ";")
dataset.red.wines$quality[which(dataset.red.wines$quality == 3 | dataset.red.wines$quality == 4)] <- 'poor'
dataset.red.wines$quality[which(dataset.red.wines$quality == 5 | dataset.red.wines$quality == 6)] <- 'ok'
dataset.red.wines$quality[which(dataset.red.wines$quality == 7 | dataset.red.wines$quality == 8)] <- 'great'
dataset.red.wines$quality <- as.factor(dataset.red.wines$quality)
set.seed(100)
# Step 1: get row numbers for the training data
training.row.numbers <- createDataPartition(dataset.red.wines$quality, p=0.8, list=FALSE)
# Step 2: create the training  dataset
dataset.training <- dataset.red.wines[training.row.numbers,]
# Step 3: create the test dataset
dataset.testing <- dataset.red.wines[-training.row.numbers,]
str(dataset.training)
# Define the training control - this problem has THREE classes so ROC is not available
fit.control <- trainControl(
method = 'cv',                   # k-fold cross validation
number = 5,                      # number of folds
savePredictions = 'final',       # saves predictions for optimal tuning parameter
classProbs = T#,                  # should class probabilities be returned
#summaryFunction=twoClassSummary  # results summary function
)
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
trControl = fit.control)
model.rf
# show the significance of predictors, in order
varImp(model.rf)
set.seed(100)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
trControl = fit.control)
model.svm
# show the significance of predictors, in order
varImp(model.svm)
# train k-nearest neighbors
model.knn = train(quality ~ .,
data=dataset.training,
method='knn',
tuneLength=5,
trControl=fit.control)
model.knn
varImp(model.knn)
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
trControl=fit.control)
model.nb
# Compare model performances using resample()
models.compared <- resamples(list(`Random Forest`=model.rf,
`SVM`=model.svm,
`kNN`=model.knn,
`Naive Bayes`=model.nb
))
# Summary of the models performances
summary(models.compared)
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
preProcess = c("center", "scale"),
trControl = fit.control)
model.rf
set.seed(100)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
preProcess = c("center", "scale"),
trControl = fit.control)
model.svm
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
preProcess = c("center", "scale"),
trControl=fit.control)
model.nb
# apply capping to dataset.training$fixed.acidity
qnt <- quantile(dataset.training$fixed.acidity, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$fixed.acidity, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$fixed.acidity, na.rm = T)
dataset.training$fixed.acidity[dataset.training$fixed.acidity < (qnt[1] - H)] <- caps[1]
dataset.training$fixed.acidity[dataset.training$fixed.acidity > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$volatile.acidity
qnt <- quantile(dataset.training$volatile.acidity, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$volatile.acidity, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$volatile.acidity, na.rm = T)
dataset.training$volatile.acidity[dataset.training$volatile.acidity < (qnt[1] - H)] <- caps[1]
dataset.training$volatile.acidity[dataset.training$volatile.acidity > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$citric.acid
qnt <- quantile(dataset.training$citric.acid, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$citric.acid, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$citric.acid, na.rm = T)
dataset.training$citric.acid[dataset.training$citric.acid < (qnt[1] - H)] <- caps[1]
dataset.training$citric.acid[dataset.training$citric.acid > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$chlorides
qnt <- quantile(dataset.training$chlorides, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$chlorides, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$chlorides, na.rm = T)
dataset.training$chlorides[dataset.training$chlorides < (qnt[1] - H)] <- caps[1]
dataset.training$chlorides[dataset.training$chlorides > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$free.sulfur.dioxide
qnt <- quantile(dataset.training$free.sulfur.dioxide, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$free.sulfur.dioxide, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$free.sulfur.dioxide, na.rm = T)
dataset.training$free.sulfur.dioxide[dataset.training$free.sulfur.dioxide < (qnt[1] - H)] <- caps[1]
dataset.training$free.sulfur.dioxide[dataset.training$free.sulfur.dioxide > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$total.sulfur.dioxide
qnt <- quantile(dataset.training$total.sulfur.dioxide, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$total.sulfur.dioxide, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$total.sulfur.dioxide, na.rm = T)
dataset.training$total.sulfur.dioxide[dataset.training$total.sulfur.dioxide < (qnt[1] - H)] <- caps[1]
dataset.training$total.sulfur.dioxide[dataset.training$total.sulfur.dioxide > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$density
qnt <- quantile(dataset.training$density, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$density, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$density, na.rm = T)
dataset.training$density[dataset.training$density < (qnt[1] - H)] <- caps[1]
dataset.training$density[dataset.training$density > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$pH
qnt <- quantile(dataset.training$pH, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$pH, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$pH, na.rm = T)
dataset.training$pH[dataset.training$pH < (qnt[1] - H)] <- caps[1]
dataset.training$pH[dataset.training$pH > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$sulphates
qnt <- quantile(dataset.training$sulphates, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$sulphates, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$sulphates, na.rm = T)
dataset.training$sulphates[dataset.training$sulphates < (qnt[1] - H)] <- caps[1]
dataset.training$sulphates[dataset.training$sulphates > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$alcohol
qnt <- quantile(dataset.training$alcohol, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$alcohol, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$alcohol, na.rm = T)
dataset.training$alcohol[dataset.training$alcohol < (qnt[1] - H)] <- caps[1]
dataset.training$alcohol[dataset.training$alcohol > (qnt[2] + H)] <- caps[2]
# apply capping to dataset.training$residual.sugar
qnt <- quantile(dataset.training$residual.sugar, probs=c(.25, .75), na.rm = T)
caps <- quantile(dataset.training$residual.sugar, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(dataset.training$residual.sugar, na.rm = T)
dataset.training$residual.sugar[dataset.training$residual.sugar < (qnt[1] - H)] <- caps[1]
dataset.training$residual.sugar[dataset.training$residual.sugar > (qnt[2] + H)] <- caps[2]
# boxplots
par(mfrow=c(1,6))
for(i in 1:6) {
boxplot(dataset.training[,i], main=names(dataset.training)[i])
}
# boxplots
par(mfrow=c(1,5))
for(i in 7:11) {
boxplot(dataset.training[,i], main=names(dataset.training)[i])
}
# re-build the models with the capped predictors
set.seed(100)
# train random forest
model.rf = train(quality ~ .,
data=dataset.training,
method='rf',
tuneLength=5,
trControl = fit.control)
# train SVM
model.svm = train(quality ~ .,
data=dataset.training,
method='svmRadial',
tuneLength=15,
trControl = fit.control)
# train k-nearest neighbors
model.knn = train(quality ~ .,
data=dataset.training,
method='knn',
tuneLength=5,
trControl=fit.control)
# train naive baise classifier
model.nb = train(quality ~ .,
data=dataset.training,
method='nb',
tuneLength=5,
trControl=fit.control)
# Compare model performances using resample()
models.compared <- resamples(list(`Random Forest`=model.rf,
`SVM`=model.svm,
`kNN`=model.knn,
`Naive Bayes`=model.nb
))
# Summary of the models performances
summary(models.compared)
#install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth'))
#install.packages('e1071')
#install.packages('kernlab')
#install.packages('ellipse')
#install.packages('PerformanceAnalytics')
#install.packages('klaR')
#install.packages('outliers')
